# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

schedules:
- cron: "7 8 * * *"
  displayName: Nightly build master
  branches:
    include:
    - master
  always: false # only run if there have been source code changes since the last successful scheduled run
- cron: "7 20 * * *"
  displayName: Nightly build staging
  branches:
    include:
    - staging
  always: true 


trigger: none

pr: none

variables:
- group: LinuxAgentPool

jobs:
- job: nightly
  displayName: 'Nightly tests Linux Spark'
  timeoutInMinutes: 180 # how long to run the job before automatically cancelling
  pool:
    name: $(Agent_Pool)

  steps:
  - bash: |
      echo "##vso[task.prependpath]/data/anaconda/bin"
      conda env list
    displayName: 'Add Conda to PATH'

  - script: |
      conda env remove -n nightly_reco_pyspark && \
      python ./tools/generate_conda_file.py --pyspark --name nightly_reco_pyspark && \
      conda env create --quiet -f nightly_reco_pyspark.yaml 2> log
    displayName: 'Setup Conda Env'

  - script: |
      . /anaconda/etc/profile.d/conda.sh && \
      conda activate nightly_reco_pyspark && \
      echo "Smoke tests" && \
      pytest tests/smoke --durations 0 -m "smoke and spark and not gpu" --junitxml=reports/test-smoke.xml && \
      echo "Integration tests" && \
      pytest tests/integration --durations 0 -m "integration and spark and not gpu" --junitxml=reports/test-integration.xml && \
      conda deactivate    
    displayName: 'Run Tests'
    env:
      PYSPARK_PYTHON: /anaconda/envs/nightly_reco_pyspark/bin/python
      PYSPARK_DRIVER_PYTHON: /anaconda/envs/nightly_reco_pyspark/bin/python

  - task: PublishTestResults@2
    displayName: 'Publish Test Results '
    inputs:
      testResultsFiles: '**/test-*.xml'
      failTaskOnFailedTests: true
    condition: succeededOrFailed()

  - script: |
      conda env remove -n nightly_reco_pyspark -y   
    workingDirectory: tests
    displayName: 'Conda remove'
    continueOnError: true
    condition: always() # this step will always run, even if the pipeline is canceled