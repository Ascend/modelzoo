alg_para:
  alg_name: QMixAlg               # The qmix algorithm defined within xingtian
  alg_config:                     # algorithm config
    batch_size: 32                # train batch size
    buffer_size: 5000             # train buffer  size of trajectory
    epsilon_anneal_time: 50000    # time_length of epsilon schedule
    epsilon_finish: 0.05          # epsilon minimum
    epsilon_start: 1.0            # epsilon maximum
    obs_agent_id: True            # use agent id within state
    obs_last_action: True         # use last action within state with onehot
    target_update_interval: 200   # interval to update target network

env_para:
  env_name: StarCraft2Xt      # The environment defined within xingtian
  env_info: {                 # follows are the starcraft simulator set
    "continuing_episode": False,
    "difficulty": "7",
    "game_version": null,
    "map_name": "2s_vs_1sc",   # map set
    "move_amount": 2,
    "obs_all_health": True,
    "obs_instead_of_state": False,
    "obs_last_action": False,
    "obs_own_health": True,
    "obs_pathing_grid": False,
    "obs_terrain_height": False,
    "obs_timestep_number": False,
    "reward_death_value": 10,
    "reward_defeat": 0,
    "reward_negative_scale": 0.5,
    "reward_only_positive": True,
    "reward_scale": True,
    "reward_scale_rate": 20,
    "reward_sparse": False,
    "reward_win": 200,
    "replay_dir": "",
    "replay_prefix": "",
    "state_last_action": True,
    "state_timestep_number": False,
    "step_mul": 8,
    "seed": null,
    "heuristic_ai": False,
    "heuristic_rest": False,
    "debug": False,
  }

agent_para:
  agent_name: StarCraftQMix     # The qmix agent defined within xingtian
  agent_num : 1                 # makeup multiagent as a whole
  agent_config: {
    'complete_step': 2050000    # explore step in total
    }

model_para:
  actor:
    model_name: QMixModel       # The qmix model defined within xingtian
    use_npu: False              # npu usage flag
    allow_mix_precision: True   # setup mix precision

    model_config:
      gamma: 0.99               # discount value for accumulative reward
      grad_norm_clip: 10        # clip value for grad norm
      hypernet_embed: 64        # size of each hypernet embed
      hypernet_layers: 2        # layers num of hupernet
      lr: 0.0005                # learning rate
      mixing_embed_dim: 32      # the dimensionality of mixing embed
      rnn_hidden_dim: 64        # the dimensionality of rnn hidden
      batch_size: 32            # train batch size for tensorboard model build
      use_double_q: True        # build model with double q algorithm

env_num: 1                      # explore environment number to parallel
node_config: [["127.0.0.1", "username", "passwd"],]
#test_node_config: [["127.0.0.1", "username", "passwd"],]

benchmark:
  id: xt_qmix
#  archive_root: ../xt_archive  # default: ~/xt_archive
  eval:
    gap: 256                      # train times call once evaluate
    evaluator_num: 1              # run eval with how much evaluator instance
    episodes_per_eval: 32         # run how much episodes within one evaluate
    max_step_per_episode: 1000
