# Train RetinaNet with COCO dataset using Cloud TPU and Google Kubernetes
# Engine.
#
# [Training Data]
#   Download and preprocess the COCO dataset using https://github.com/tensorflow/tpu/blob/r1.13/tools/datasets/download_and_preprocess_coco_k8s.yaml
#   if you don't already have the data.
#
# [Instructions]
#   1. Follow the instructions on https://cloud.google.com/tpu/docs/kubernetes-engine-setup
#      to create a Kubernetes Engine cluster.
#   2. Change the environment variable TRAIN_FILE_PATTERN and MODEL_BUCKET below to the
#      Google Cloud Storage location where you downloaded the COCO dataset and
#      where you want to store the output model, respectively. For running eval as well
#      VAL_JSON_FILE and EVAL_FILE_PATTERN.
#   3. Run `kubectl create -f retinanet_k8s.yaml`.

apiVersion: batch/v1
kind: Job
metadata:
  name: retinanet-tpu
spec:
  template:
    metadata:
      annotations:
        # The Cloud TPUs that will be created for this Job must support
        # TensorFlow 1.13. This version MUST match the TensorFlow version that
        # your model is built on.
        tf-version.cloud-tpus.google.com: "1.13"
    spec:
      restartPolicy: Never
      containers:
      - name: retinanet-tpu
        # The official TensorFlow 1.13 TPU model image built from:
        # https://github.com/tensorflow/tpu/blob/r1.13/tools/docker/Dockerfile.
        image: gcr.io/tensorflow/tpu-models:r1.13
        command:
          - /bin/sh
          - -c
          - >
            DEBIAN_FRONTEND=noninteractive apt-get update &&
            DEBIAN_FRONTEND=noninteractive apt-get install -y python-dev python-tk &&
            pip install Cython matplotlib &&
            pip install 'git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI' &&
            python /tensorflow_tpu_models/models/official/detection/main.py
            --use_tpu=True
            --model_dir="${MODEL_DIR?}"
            --num_cores=8
            --mode=train
            --eval_after_training=True
            --params_override="{ type: retinanet, train: { checkpoint: { path: ${RESNET_CHECKPOINT?},
             prefix: resnet50/ }, train_file_pattern: ${TRAIN_FILE_PATTERN?} },
             eval: { val_json_file: ${VAL_JSON_FILE?}, eval_file_pattern: ${EVAL_FILE_PATTERN?},
             eval_samples: 5000 } }"
        env:
          # [REQUIRED] Must specify the Google Cloud Storage location where the
          # training data is stored.
        - name: TRAIN_FILE_PATTERN
          value: "gs://<data_bucket>/coco/train*"
          # [REQUIRED] Must specify the Google Cloud Storage location where the
          # model and the checkpoint will be stored.
        - name: MODEL_DIR
          value: "gs://<my-model-bucket>/retinanet"
          # RetinaNet requires a pre-trained image classification model (like
          # ResNet) as a backbone network. This example uses a pretrained
          # checkpoint created with the ResNet demonstration model. You can
          # instead train your own ResNet model if desired, and specify a
          # checkpoint from your ResNet model directory.
        - name: RESNET_CHECKPOINT
          value: "gs://cloudtpu-coco-data/pretrain/resnet50-checkpoint-2018-02-07"
        - name: VAL_JSON_FILE
          value: "gs://<data_bucket>/coco/instances_val2017.json"
        - name: EVAL_FILE_PATTERN
          value: "gs://<data_bucket>/coco/val*"
        resources:
          limits:
            # Request a single v3-8 Cloud TPU device to train the model.
            # A single v3-8 Cloud TPU device consists of 4 chips, each of which
            # has 2 cores, so there are 8 cores in total.
            cloud-tpus.google.com/v3: 8
