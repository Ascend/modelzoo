general:
    worker:
        devices_per_job: -1
    backend: tensorflow
    device_category: NPU

pipeline: [nas]
#pipeline: [nas, fully_train]

nas:
    pipe_step:
        type: NasPipeStep

    dataset:
        type: Cifar10
        common:
            data_path: /cache/datasets/cifar-10-batches-bin
            batch_size: 64
            num_parallel_batches: 64
            fp16: False

    search_algorithm:
        type: QuantEA
        codec: QuantCodec
        policy:
            length: 40
            num_generation: 2
            num_individual: 2
            random_models: 1

    search_space:
        type: SearchSpace
        modules: ['backbone']
        bit_candidates: [4, 8]
        backbone:
            name: 'QuantResNet'
            num_blocks: [3, 3, 3]
            num_classes: 10

    trainer:
        type: Trainer
        callbacks: QuantTrainerCallback
        epochs: 1
        optim:
            type: MomentumOptimizer
            params:
                learning_rate: 0.1
                momentum: 0.9
        lr_scheduler:
            type: MultiStepLrWarmUp
            params:
                base_lr: 0.1
                warmup: False
                milestones: [20, 40, 60, 80, 100]
                decay_rates: [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]
        loss:
            type: CrossEntropyWeightDecay
            params:
                cross_entropy: sparse_softmax_cross_entropy
                weight_decay: !!float 1e-4
        seed: 10
        save_steps: 250
        distributed: False
        amp: True
        limits:
            flop_range: [!!float 0, !!float 1e10]

fully_train:
    pipe_step:
        type: FullyTrainPipeStep
        models_folder: "{local_base_path}/output/nas/"
    dataset:
        ref: nas.dataset

    search_space:
        type: SearchSpace
        modules: ['backbone']
        bit_candidates: [4, 8]
        backbone:
            name: 'QuantResNet'
            num_blocks: [3, 3, 3]
            num_classes: 10

    trainer:
        ref: nas.trainer
        lr_scheduler:
            type: MultiStepLrWarmUp
            params:
                base_lr: 0.1
                warmup: False
                milestones: [20, 40, 60, 80, 100]
                decay_rates: [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]
